{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "import re\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from glob import glob\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from collections import defaultdict, deque\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses asynchronous Python libraries to query the Scopus API's effeciently and retrieve the following:\n",
    "\n",
    "1. Affiliation codes associated with the string \"George Washington University\"\n",
    "\n",
    "2. Article search results where any of the authors are tagged with any of those affiliations\n",
    "\n",
    "3. Author profile information for each author in the above associated with those affiliations\n",
    "\n",
    "Results are retrieved in XML format and parsed using the lxml library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is necessary to make the aiohttp requests in rate-limited fashion.\n",
    "# The Scopus API's have rate limits associated with them; exceeding those limits will raise errors.\n",
    "# From https://github.com/hallazzang/asyncio-throttle\n",
    "class Throttler:\n",
    "    def __init__(self, rate_limit, period=1.0, retry_interval=0.01):\n",
    "        self.rate_limit = rate_limit\n",
    "        self.period = period\n",
    "        self.retry_interval = retry_interval\n",
    "\n",
    "        self._task_logs = deque()\n",
    "\n",
    "    def flush(self):\n",
    "        now = time.time()\n",
    "        while self._task_logs:\n",
    "            if now - self._task_logs[0] > self.period:\n",
    "                self._task_logs.popleft()\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    async def acquire(self):\n",
    "        while True:\n",
    "            self.flush()\n",
    "            if len(self._task_logs) < self.rate_limit:\n",
    "                break\n",
    "            await asyncio.sleep(self.retry_interval)\n",
    "\n",
    "        self._task_logs.append(time.time())\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        await self.acquire()\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc, tb):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the above \n",
    "#The code should run significantly faster by replacing the argument throttler with a factor of 10, e.g., 100\n",
    "async def test_throttle(throttler, i):\n",
    "    async with throttler:\n",
    "        return i\n",
    "async def run_test(loop):\n",
    "    throttler = Throttler(rate_limit=10)\n",
    "    tasks = [loop.create_task(test_throttle(throttler, i)) for i in range(100)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "loop = asyncio.get_event_loop()\n",
    "results = loop.run_until_complete(run_test(loop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCOPUS data ####\n",
    "\n",
    "Retrieving data from the Elsevier developer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **headers** contain the API key (from https://dev.elsevier.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SCOP_HEADERS = {'Accept': 'application/xml',\n",
    "                 'X-ELS-APIKey' : ''}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Affiliation search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fields to retrieve:**\n",
    "- affiliation-name\n",
    "- dc:identifier (=afid in Scopus search)\n",
    "- document-count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# These are passed to the requests.get function\n",
    "params = {'query': 'affil({george washington university})',\n",
    "         'field': 'affiliation-name,dc:identifier,document-count'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_url = 'http://api.elsevier.com/content/search/affiliation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We join the keys and values from the dict with the equals sign and separate each entry with the ampersand\n",
    "# This code is NOT asynchronous -- we're just making one request here\n",
    "r = requests.get(base_url, headers=SCOP_HEADERS, params='&'.join([k+'='+v for k,v in params.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Response = 200: good\n",
    "# TO DO --> error handling on requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_xml(xmldoc):\n",
    "    '''Returns an ElementTree root Element and a namespace map'''\n",
    "    # load the XML response body\n",
    "    parser = etree.XMLParser(encoding='utf-8')\n",
    "    # Get rid of the default namespace\n",
    "    xmldoc = re.sub(r'\\sxmlns=\"[^\"]+\"', '', xmldoc, count=1)\n",
    "    # Need to use the explicit encoding in order to avoid errors\n",
    "    tree = etree.fromstring(xmldoc.encode('utf8'), parser=parser)\n",
    "    return tree, tree.nsmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parse the affiliation search result\n",
    "tree, _ = parse_xml(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Output CSV for customization of affiliations\n",
    "# We will use the affiliation codes to find GW authors\n",
    "aff_tbl = pd.DataFrame.from_dict(\n",
    "    dict(\n",
    "        zip(\n",
    "            [node.text for node in tree.xpath('//affiliation-name')],\n",
    "[node.text for node in tree.xpath('//dc:identifier', namespaces=tree.nsmap)]\n",
    "        )\n",
    "    ), orient='index',\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>affiliation_name</th>\n",
       "      <th>affiliation_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George Washington University</td>\n",
       "      <td>60003088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>George Washington University Medical Center</td>\n",
       "      <td>100931046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>George Washington University School of Medicin...</td>\n",
       "      <td>113202420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>George Washington University Hospital</td>\n",
       "      <td>100851611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elliott School of International Affairs</td>\n",
       "      <td>60077343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>District of Columbia General Hospital</td>\n",
       "      <td>60001971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>George Washington University School of Public ...</td>\n",
       "      <td>60032302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>D.C. General Hospital</td>\n",
       "      <td>60009393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>National Capital Poison Center</td>\n",
       "      <td>60031795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>George Washington University Biostatistics Center</td>\n",
       "      <td>101758582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>National Security Archive</td>\n",
       "      <td>60024161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Jacobs Institute for Women's Health</td>\n",
       "      <td>60012814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>George Washington University Center for the Ad...</td>\n",
       "      <td>60077295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Banco Central do Brasil</td>\n",
       "      <td>100611187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Washington University Law School</td>\n",
       "      <td>112974921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>George Washington University School of Medicine</td>\n",
       "      <td>105386412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The George Washington University Medical Center</td>\n",
       "      <td>121348994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>George Washington University - JIAFS</td>\n",
       "      <td>100965439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     affiliation_name affiliation_id\n",
       "0                        George Washington University       60003088\n",
       "1         George Washington University Medical Center      100931046\n",
       "2   George Washington University School of Medicin...      113202420\n",
       "3               George Washington University Hospital      100851611\n",
       "4             Elliott School of International Affairs       60077343\n",
       "5               District of Columbia General Hospital       60001971\n",
       "6   George Washington University School of Public ...       60032302\n",
       "7                               D.C. General Hospital       60009393\n",
       "8                      National Capital Poison Center       60031795\n",
       "9   George Washington University Biostatistics Center      101758582\n",
       "10                          National Security Archive       60024161\n",
       "11                Jacobs Institute for Women's Health       60012814\n",
       "12  George Washington University Center for the Ad...       60077295\n",
       "13                            Banco Central do Brasil      100611187\n",
       "14                   Washington University Law School      112974921\n",
       "15    George Washington University School of Medicine      105386412\n",
       "16    The George Washington University Medical Center      121348994\n",
       "17               George Washington University - JIAFS      100965439"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aff_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aff_tbl.columns=['affiliation_name', 'affiliation_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now get rid of the extraneous text on the ID column\n",
    "aff_tbl.affiliation_id = aff_tbl.affiliation_id.apply(lambda x: x.split(':')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the file to disk\n",
    "aff_tbl.to_csv('../data/scholcomm/faculty_pubs/affiliation_tbl_092018.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Document retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fields to include:**\n",
    "\n",
    "https://dev.elsevier.com/guides/ScopusSearchViews.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read a table of affiliations we want to use\n",
    "# This table may be modified in a spreadsheet or text editor to exclude unwanted affiliation codes\n",
    "aff_tbl = pd.read_csv('../data/scholcomm/faculty_pubs/affiliation_tbl_092018.csv',\n",
    "                     dtype={'affiliation_id': 'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BASE_URL = 'http://api.elsevier.com/content/search/scopus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Where we want to save the data\n",
    "DATA_DIR = '../data/scholcomm/faculty_pubs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dump_xml(doc_list, path):\n",
    "    '''Accepts a list of XML documents and a path (pathlib object) to a folder to write them to.'''\n",
    "    # Store the XML output to disk\n",
    "    for i, d in enumerate(doc_list):\n",
    "        with open(path / 'results_{}.xml'.format(i), 'w', errors='ignore') as f:\n",
    "            f.write(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xml(path):\n",
    "    '''Accepts a path (pathlib object) to a folder of XML files to load. Returns the XML docs as a list.'''\n",
    "    doc_list = []\n",
    "    for p in path.glob('*.xml'):\n",
    "        with open(p, 'r') as f:\n",
    "            doc_list.append(f.read())\n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To DO:\n",
    "# 1. Use cursor to retrieve more than 5000 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def do_request(url, params, throttler, start=None):\n",
    "    '''Using the aiohttp library to make asynchronous requests. Accepts an instance of the Throttler class (defined above) \n",
    "    with a rate limit set. This will keep the requests bounded by the rate limit.'''\n",
    "    async with throttler:\n",
    "        async with aiohttp.ClientSession() as client:\n",
    "            # Need to update the params here...updating them outside of the with statement for some reason doesn't work\n",
    "            if start:\n",
    "                # On each pass, the \"start\" query parameter will be updated when working through a batch of requests\n",
    "                params['start'] = str(start)\n",
    "            async with client.get(url, \n",
    "                             headers=SCOP_HEADERS, \n",
    "                             params='&'.join([k+'='+v for k,v in params.items()])) as session:\n",
    "                response = await session.text()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def batched_search(num_results, params, loop, start=25, rate_limit=6):\n",
    "    '''Helper for the Scopus Search query. Can be used to restart a search interrupted, given an offset number'''\n",
    "    # Create the iterable of request tasks, where each task is an async request (an awaitable)    \n",
    "    # The first time through, assume we have retrieved the first 25 results\n",
    "    # Using the asyncio_throttler library to limit our requests to X number per second, as per the Scopus API specs\n",
    "    throttler = Throttler(rate_limit=rate_limit)\n",
    "    # Run the requests concurrently\n",
    "    awaitables = [loop.create_task(do_request(BASE_URL, params, throttler, i)) for i in range(start, int(num_results), 25)]\n",
    "    results = await asyncio.gather(*awaitables)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_works_search(params):\n",
    "    '''Given a set of affiliations and a date range, returns all the results (works) for that query\n",
    "       on the Scopus Search API'''\n",
    "    # Using the complete view, we can get only 25 results at a time.\n",
    "    # So we iterate in batches\n",
    "    # Store the results in a running list\n",
    "    doc_list = []\n",
    "    # Perform an initial query to get the number of results\n",
    "    try:\n",
    "        r = requests.get(BASE_URL, \n",
    "                         headers=SCOP_HEADERS, \n",
    "                         params='&'.join([k+'='+v for k,v in params.items()]))\n",
    "        if r.status_code != 200:\n",
    "            # Throw error on connection failure --> TO DO: implement logging\n",
    "            raise Exception(r.text)\n",
    "        tree, nsmap = parse_xml(r.text)\n",
    "        # Get the total number of results\n",
    "        num_results = tree.find('opensearch:totalResults', \n",
    "                                    namespaces=nsmap).text\n",
    "        doc_list.append(r.text)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    # Initialize the event loop for the asynchronous queries\n",
    "    loop = asyncio.get_event_loop()\n",
    "    print(\"Retrieving {} results\".format(num_results))\n",
    "    #loop.run_untiL_complete will pass along the return value of the called function\n",
    "    doc_list.extend(loop.run_until_complete(batched_search(num_results, params, loop)))\n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_works(tree, nsmap, aff_xpath_strs):\n",
    "    '''Parses a page of search results, extracting metadata and affiliated authors'''\n",
    "    # Dictionaries to look up a) a list of works by Author ID and b) a list of authors by Work ID\n",
    "    work_authors = defaultdict(list)\n",
    "    author_works = defaultdict(list)\n",
    "    # The list of article-level metadata for each work\n",
    "    works = []    \n",
    "    # Iterate over the entries in the XML list of results\n",
    "    for entry in tree.xpath('//entry', namespaces=nsmap):\n",
    "        # Unique Scopus work ID\n",
    "        work_id = entry.find('dc:identifier', namespaces=nsmap).text\n",
    "        # Authors whose affiliations match those in our list\n",
    "        # B/c the Scopus search only returns at most 100 authors per work, affiliated authors may be missed if \n",
    "        #    a) the list of authors has > 100 elements and \n",
    "        #    b) the affiliated author's place in the sequence is > 100\n",
    "        aff_authors = entry.xpath('author[{}]'\n",
    "                                    .format(aff_xpath_strs['afid']), \n",
    "                                            namespaces=nsmap)\n",
    "        # For each author, create a mapping between the article ID and the Scopus author ID\n",
    "        # This will be useful for connecting up the more specific affiliation info we can get from the author profiles\n",
    "        for auth_elem in aff_authors:\n",
    "            auth_url = auth_elem.find('author-url', namespaces=nsmap).text\n",
    "            # The author's ID is at the end of the URL to their profile\n",
    "            auth_id = auth_url.split('/')[-1]\n",
    "            work_authors[work_id].append(auth_url)\n",
    "            author_works[auth_id].append(work_id)\n",
    "        # Extract the rest of the citation metadata, exclusive of the authors\n",
    "        # QName.localname drops the namespace prefix\n",
    "        work = {etree.QName(element).localname: element.text for element \n",
    "                       in entry.xpath('dc:*|prism:*', namespaces=nsmap)}\n",
    "            # Add the complete list of authors\n",
    "        work['authors'] = [element.text for element \n",
    "                            in entry.xpath('author/authname', namespaces=tree.nsmap)]\n",
    "        # Add the cited-by count\n",
    "        work['citedby_count'] = entry.find('citedby-count', namespaces=tree.nsmap).text\n",
    "        # Store this work in the list\n",
    "        works.append(work)\n",
    "    return (work_authors, author_works, works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This string allows us to limit our search to those works associated with the affiliations in our list\n",
    "aff_str = '+or+'.join(['af-id({})'.format(afid) for afid in aff_tbl.affiliation_id])\n",
    "# Start year should be one less than the desired year, e.g., enter 2017 to find everything published in 2018 or more recently\n",
    "start_year = 2018\n",
    "params = {'query': '({})+PUBYEAR+>+{}'.format(aff_str, \n",
    "                            str(start_year)),\n",
    "                         'view': 'complete',\n",
    "                          'start': '0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving 465 results\n"
     ]
    }
   ],
   "source": [
    "doc_list = run_full_works_search(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String for efficient XPATH queries on the affiliation ids we're looking for\n",
    "# Allows us to ignore co-authors not associated with our affiliations\n",
    "aff_xpath_strs = {'afid': ' or '.join(['afid=\"{}\"'.format(a) for a in aff_tbl.affiliation_id]),\n",
    "                               'parent_id': ' or '.join(['@parent=\"{}\"'.format(a) for a in aff_tbl.affiliation_id])}\n",
    "works = []\n",
    "author_works = {}\n",
    "work_authors = {}\n",
    "# Process each page of results\n",
    "for d in doc_list:\n",
    "    tree, nsmap = parse_xml(d)\n",
    "    work_authors_batch, author_works_batch, works_batch = parse_works(tree, nsmap, aff_xpath_strs)\n",
    "    works.extend(works_batch)\n",
    "    work_authors.update(work_authors_batch)\n",
    "    author_works.update(author_works_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: the number of parsed works should equal the number reported by our initial search\n",
    "assert len(works) == 465"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_num': 2, 'id': 'SCOPUS_ID:85061593910'},\n",
       " {'doc_num': 2, 'id': 'SCOPUS_ID:85060016891'},\n",
       " {'doc_num': 4, 'id': 'SCOPUS_ID:85060236337'},\n",
       " {'doc_num': 7, 'id': 'SCOPUS_ID:85052723351'},\n",
       " {'doc_num': 11, 'id': 'SCOPUS_ID:85059443714'},\n",
       " {'doc_num': 12, 'id': 'SCOPUS_ID:85057551299'},\n",
       " {'doc_num': 14, 'id': 'SCOPUS_ID:85061055309'},\n",
       " {'doc_num': 14, 'id': 'SCOPUS_ID:85059232340'},\n",
       " {'doc_num': 17, 'id': 'SCOPUS_ID:85058847142'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test: are all the documents captured in our author list? If not, why not?\n",
    "id_list = []\n",
    "for i, d in enumerate(doc_list):\n",
    "    tree, nsmap = parse_xml(d)\n",
    "    ids = tree.xpath('//entry/dc:identifier/text()', namespaces=nsmap)\n",
    "    id_list.extend([{'doc_num': i, 'id': t} for t in ids])\n",
    "[i for i in id_list if i['id'] not in work_authors] # Sould return an empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pathlib library to create platform-specific paths\n",
    "# Helps simplify navigating the filesystem\n",
    "path_to_tests = Path(DATA_DIR) / 'tests/scopus_search'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_xml(doc_list, path_to_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Retrieve author profiles for each author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URL's for the author profiles in this set of works\n",
    "# Using a set since the same author may be attached to more than one work\n",
    "author_urls = list({url for urls in work_authors.values() for url in urls})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Authors API, the only necessary parameter is the type of view\n",
    "# The URL contains the author ID, so we need to make one request per author\n",
    "throttler = Throttler(rate_limit=2)\n",
    "params={'view': 'STANDARD'}\n",
    "# Testing a small batch\n",
    "loop = asyncio.get_event_loop()\n",
    "awaitables = [loop.create_task(do_request(url, params, throttler)) for url in author_urls]\n",
    "results = await asyncio.gather(*awaitables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tests = Path(DATA_DIR) / 'tests/author_search'\n",
    "dump_xml(results, path_to_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [parse_xml(r) for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = [parse_author_profile(r[0], r[1], aff_tbl, aff_xpath_strs) for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auth_id': '6603418598',\n",
       " 'index_name': 'Samango-Sprouse C.',\n",
       " 'surname': 'Samango-Sprouse',\n",
       " 'given-name': 'Carole A.',\n",
       " 'departments': [{'Department': 'Department of Pediatrics',\n",
       "   'Type': 'Current',\n",
       "   'Parent': 'George Washington University'}]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles[275]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_author_profile(tree, nsmap, aff_tbl, aff_xpath_strs):\n",
    "    '''Parses an author profile, extracting name and relevant affiliations'''\n",
    "    author = {'auth_id': tree.xpath('//dc:identifier', \n",
    "                                      namespaces=nsmap)[0].text.split(':')[1]}\n",
    "    #Get the indexed name as well as surname and given name\n",
    "    author['index_name'] = tree.find('author-profile/preferred-name/indexed-name').text\n",
    "    author['surname'] = tree.find('author-profile/preferred-name/surname').text\n",
    "    author['given-name'] = tree.find('author-profile/preferred-name/given-name').text\n",
    "    # List to store the author's affiliations\n",
    "    author['departments'] = []\n",
    "    # Get the value of the author's current affiliation\n",
    "    # Need to loop through the current affiliations, which may contain more than one\n",
    "    affs_current = tree.xpath('//affiliation-current/affiliation')\n",
    "    for aff in affs_current:\n",
    "        aff_current_id = aff.get('affiliation-id')\n",
    "        aff_current_parent = aff.get('parent')\n",
    "        # Is this author affiliation in our target list?\n",
    "        if (aff_current_id in aff_tbl.affiliation_id.values) \\\n",
    "            or (aff_current_parent in aff_tbl.affiliation_id.values):\n",
    "            # Capture the preferred name listed for the current department\n",
    "            current_dept = {'Department': aff.find('ip-doc/preferred-name').text,\n",
    "                                   'Type': 'Current'}\n",
    "            # If there's a parent entity listed, capture that, too\n",
    "            parent_dept = aff.find('ip-doc/parent-preferred-name')\n",
    "            if parent_dept is not None:\n",
    "                current_dept['Parent'] = parent_dept.text\n",
    "            author['departments'].append(current_dept)\n",
    "        # If it's an author who is possibly no longer affiliated\n",
    "        #author['other_depts'] = [e.text for e in tree.xpath('//affiliation[{}]/ip-doc/preferred-name'.format(aff_xpath_strs['parent_id']))]\n",
    "    return author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
